syntax = "proto3";

package LlamaCpp;

message Void {}

message NewContextArgs {
  reserved 1; // TODO(crasm): model ref

  optional uint32 seed = 2;
  optional uint32 n_ctx = 3;
  optional uint32 n_batch = 4;
  optional uint32 n_threads = 5;
  optional uint32 n_threads_batch = 6;
  optional int32 rope_scaling_type = 7;
  optional float rope_freq_base = 8;
  optional float rope_freq_scale = 9;
  optional float yarn_ext_factor = 10;
  optional float yarn_attn_factor = 11;
  optional float yarn_beta_fast = 12;
  optional float yarn_beta_slow = 13;
  optional uint32 yarn_orig_ctx = 14;
  optional int32 type_k = 15;
  optional int32 type_v = 16;
  optional bool embedding = 17;
  optional bool offload_kqv = 18;
}

message FreeContextArgs {
  int32 ctx = 1;
}

message AddTextArgs {
  int32 ctx = 1;
  string text = 2;
}

message TrimArgs {
  int32 ctx = 1;
  int32 length = 2;
}

message IngestArgs {
  int32 ctx = 1;
}

message GenerateArgs {
  int32 ctx = 1;
}

service LlamaCpp {
  rpc NewContext(NewContextArgs) returns (NewContextResp) {};
  rpc FreeContext(FreeContextArgs) returns (Void);
  rpc AddText(AddTextArgs) returns (AddTextResp);
  rpc Trim(TrimArgs) returns (Void);
  rpc Ingest(IngestArgs) returns (Void); // TODO(crasm): return IngestProgressEvent stream
  rpc Generate(GenerateArgs) returns (stream Token); // TODO(crasm): samplers
}

message NewContextResp {
  int32 ctx = 1;
}

message AddTextResp {
  repeated Token toks = 1;
}

message Token {
  int32 id = 1;
  optional string text = 2;
}
